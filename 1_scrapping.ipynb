{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T08:01:57.467154Z",
     "start_time": "2019-11-09T08:01:56.827208Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "#%timeit ,  line_profiler, memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T19:30:44.695423Z",
     "start_time": "2019-10-30T19:30:44.691094Z"
    }
   },
   "source": [
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T08:01:57.482624Z",
     "start_time": "2019-11-09T08:01:57.469921Z"
    }
   },
   "outputs": [],
   "source": [
    "class Aux:\n",
    "    INFO = 0\n",
    "    DEBUG = 1\n",
    "    WARN = 2\n",
    "    ERROR = 3\n",
    "    NONE = 4\n",
    "    def timestamp():\n",
    "        now = datetime.now()\n",
    "        return ''.join(str(datetime.timestamp(now)).split('.'))\n",
    "\n",
    "    def string(string, var):\n",
    "        return string % var\n",
    "    \n",
    "    def get_state(res, level=4):\n",
    "        if level <= Aux.DEBUG:\n",
    "            print(\"## Status code = \"+str(res.status_code))\n",
    "            #print(\"## Message = \"+str(res.text)[:20])\n",
    "        if res.status_code >=200 and res.status_code <300:\n",
    "            print(\"  This is OK\")\n",
    "        if res.status_code >=300 and res.status_code <400:\n",
    "            print(\"  This is a redirection\")\n",
    "        if res.status_code >=400 and res.status_code <500:\n",
    "            print(\"  This is a Client problem\")\n",
    "        if res.status_code >=500 and res.status_code <600:\n",
    "            print(\"  This is a Server problem\")\n",
    "        if level == Aux.INFO:\n",
    "            print(\"#HEADERS#\", res.headers)\n",
    "            print(\"#CONTENT#\", res.content)\n",
    "            #print(\"#JSON#\", res.json())\n",
    "            \n",
    "    #hash - a md5 digest of the ts parameter, your private key and your public key (e.g. md5(ts+privateKey+publicKey)\n",
    "    def hash_md5(string):\n",
    "        return hashlib.md5(string.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def image(url):\n",
    "        return Image(url, width=100, height=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T08:01:57.910548Z",
     "start_time": "2019-11-09T08:01:57.486361Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T08:01:57.936350Z",
     "start_time": "2019-11-09T08:01:57.913578Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def parser2(content):\n",
    "    soup = BeautifulSoup(content,'lxml')\n",
    "    div_search = soup.find_all('div',{'class':'q1'})\n",
    "    divs = [span for span in div_search]\n",
    "    span_search = soup.find_all('span',{'class':['label','content']})\n",
    "    spans = [span.text for span in span_search if len(span.string)>1 \n",
    "             or str(span.string).isnumeric()]  \n",
    "    return spans\n",
    "\n",
    "class Spider:\n",
    "    def __init__(self, url_pattern, pages_to_scrape=1, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "        self.results = []\n",
    "        self.responses = []\n",
    "        self.df = pd.DataFrame()\n",
    "        parent, leaf = url_pattern.rsplit(\"/\", maxsplit=1)\n",
    "        book, version = leaf.rsplit(\".\", maxsplit=1)\n",
    "        self.book = book[:3]\n",
    "        self.version = version\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url, chapter):\n",
    "        try:\n",
    "            #print('url:',url)\n",
    "            header = self.get_random_ua()\n",
    "            #print('header:',header)\n",
    "            response = requests.get(url,headers=header)\n",
    "            #Aux.get_state(response, Aux.DEBUG)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print('Timeout Error')\n",
    "            pass\n",
    "        except requests.exceptions.SSLError:\n",
    "            print('SSLError Error')\n",
    "            pass\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print('RequestException Error')\n",
    "            pass\n",
    "        \n",
    "        result = self.content_parser(response.content)\n",
    "        self.to_df(result, chapter)\n",
    "        #self.responses.append(response.content)\n",
    "        #self.results.append(result)\n",
    "        self.output_results(self.book+\":\"+str(chapter))\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        sleepy = self.sleep_interval\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            if sleepy < 0:\n",
    "                sleepy =  random.random() * 1.2\n",
    "            time.sleep(sleepy)\n",
    "            self.scrape_url(self.url_pattern % i, i)\n",
    "            \n",
    "    def save(self, name='', pre=''):\n",
    "        if not name:\n",
    "            name= str(pre) + self.book + \".csv\"\n",
    "        else:\n",
    "            name = 'data/'+str(pre) + name\n",
    "        self.df.to_csv(name,index=False)\n",
    "        \n",
    "    \"\"\" returns random User Agent\"\"\"       \n",
    "    def get_random_ua(self):\n",
    "        heads =[\n",
    "            'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Mobile Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:67.0) Gecko/20100101 Firefox/67.0',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.1 Safari/605.1.15'\n",
    "        ]\n",
    "        headers = {'User-Agent': ''}\n",
    "        sample = random.sample(heads,1)\n",
    "        headers['User-Agent'] = sample[0]\n",
    "        return headers\n",
    "    \n",
    "    def to_df(self, result, chapter):\n",
    "        book = self.book\n",
    "        version = self.version\n",
    "        dic = {}\n",
    "        txt = ''\n",
    "        key = ''\n",
    "        for f in result:\n",
    "            if f.isnumeric():\n",
    "                if txt!='' and key != '':dic[key]=[book,chapter,key,txt,version]\n",
    "                key = int(f)\n",
    "                txt = ''\n",
    "            else:\n",
    "                txt += f\n",
    "        df = pd.DataFrame.from_dict(dic,orient='index',\n",
    "                            columns=['book','chapter','verse','text','version'])\n",
    "        self.df = pd.concat([self.df,df],axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T08:01:57.943640Z",
     "start_time": "2019-11-09T08:01:57.938302Z"
    }
   },
   "outputs": [],
   "source": [
    "# URL_PATTERN = 'https://my.bible.com/ca/bible/335/PRO.%s.BCI'\n",
    "# catalÃ : URL_BASE = 'https://my.bible.com/ca/bible/335/'\n",
    "URL_BASE = 'https://my.bible.com/ca/bible/1637/'\n",
    "\n",
    "# book, chapters, prefix ('PRO',31,'20'),('WIS',19,'43'),\n",
    "books = [('GEN',50,'1'),('REV',22,'1')]\n",
    "version = 'NVI'\n",
    "\n",
    "def init(books, version):\n",
    "    for (book, chapters, pre) in books:\n",
    "        URL_PATTERN = URL_BASE + book + \".%s.\" + version\n",
    "        spider = Spider(URL_PATTERN, chapters, content_parser=parser2)\n",
    "        spider.kickstart()\n",
    "        spider.save(pre= pre+'_')\n",
    "        \n",
    "#init(books, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T08:04:07.418985Z",
     "start_time": "2019-11-09T08:01:57.946689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN:1\n",
      "GEN:2\n",
      "GEN:3\n",
      "GEN:4\n",
      "GEN:5\n",
      "GEN:6\n",
      "GEN:7\n",
      "GEN:8\n",
      "GEN:9\n",
      "GEN:10\n",
      "GEN:11\n",
      "GEN:12\n",
      "GEN:13\n",
      "GEN:14\n",
      "GEN:15\n",
      "GEN:16\n",
      "GEN:17\n",
      "GEN:18\n",
      "GEN:19\n",
      "GEN:20\n",
      "GEN:21\n",
      "GEN:22\n",
      "GEN:23\n",
      "GEN:24\n",
      "GEN:25\n",
      "GEN:26\n",
      "GEN:27\n",
      "GEN:28\n",
      "GEN:29\n",
      "GEN:30\n",
      "GEN:31\n",
      "GEN:32\n",
      "GEN:33\n",
      "GEN:34\n",
      "GEN:35\n",
      "GEN:36\n",
      "GEN:37\n",
      "GEN:38\n",
      "GEN:39\n",
      "GEN:40\n",
      "GEN:41\n",
      "GEN:42\n",
      "GEN:43\n",
      "GEN:44\n",
      "GEN:45\n",
      "GEN:46\n",
      "GEN:47\n",
      "GEN:48\n",
      "GEN:49\n",
      "GEN:50\n",
      "REV:1\n",
      "REV:2\n",
      "REV:3\n",
      "REV:4\n",
      "REV:5\n",
      "REV:6\n",
      "REV:7\n",
      "REV:8\n",
      "REV:9\n",
      "REV:10\n",
      "REV:11\n",
      "REV:12\n",
      "REV:13\n",
      "REV:14\n",
      "REV:15\n",
      "REV:16\n",
      "REV:17\n",
      "REV:18\n",
      "REV:19\n",
      "REV:20\n",
      "REV:21\n",
      "REV:22\n"
     ]
    }
   ],
   "source": [
    "init(books, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
